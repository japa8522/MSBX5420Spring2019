{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 lecture: more about RDDs\n",
    "\n",
    "## Object-oriented programming: intro to `classes`\n",
    "\n",
    "When calling a function in Python, we use syntax like the following\n",
    "```\n",
    "min(5, 7)\n",
    "```\n",
    "But RDDs have this funny `.function()` syntax instead:\n",
    "```\n",
    "rdd2 = rdd1.count()\n",
    "```\n",
    "The function `count()` seems to live \"inside\" `rdd1`.\n",
    "\n",
    "We can do this because Python is an *object-oriented* language.  Let's see what this means.\n",
    "\n",
    "So far we have seen built-in types such as `int`, `float`, `str`, `list`, `dict`, and `tuple`.  In an object-oriented language we can actually make our own types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A \"class\" is a blueprint for a new type.\n",
    "class Dog(object):\n",
    "    \n",
    "    # this is the constructor\n",
    "    def __init__(self, breed, name):\n",
    "        self.breed = breed  # these are variables that live \"inside\" the object\n",
    "        self.name = name  # ...\n",
    "        print(\"Birthing a new \" + self.breed)\n",
    "        \n",
    "    # this function lives \"inside\" the object.\n",
    "    # functions like these are sometimes called \"methods\" of \"member functions\"\n",
    "    def speak(self):\n",
    "        if self.breed == \"chihuahua\":\n",
    "            print(self.name + \" says nip nip\")\n",
    "        else:\n",
    "            print(self.name + \" says bow wow\")\n",
    "        \n",
    "    # this is the destructor\n",
    "    def __del__(self):\n",
    "        print(self.name + \" is dying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birthing a new hound\n"
     ]
    }
   ],
   "source": [
    "barney = Dog(\"hound\", \"Barney\")  # this creates an \"object\" or an \"instance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barney says bow wow\n"
     ]
    }
   ],
   "source": [
    "barney.speak()  # NOTICE the . notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Barney is dying\n"
     ]
    }
   ],
   "source": [
    "del barney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birthing a new chihuahua\n"
     ]
    }
   ],
   "source": [
    "annoying = Dog(\"chihuahua\", \"Frank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank says nip nip\n"
     ]
    }
   ],
   "source": [
    "annoying.speak()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frank is dying\n"
     ]
    }
   ],
   "source": [
    "del annoying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects are everywhere in Python.  Actually, even the simple built-in types `int`, `float`, `str`, `dict`, `list`, and `tuple` are objects.\n",
    "\n",
    "## Classes in Pyspark\n",
    "\n",
    "In Pyspark the first class that you will encounter is `SparkContext`.  When you call it like a function `SparkContext(stuff...)` Python uses the class blueprint to create a new object (aka instance) in memory.  Now it is \"alive\".\n",
    "\n",
    "So that the object can refer to itself, Python names it `self` and passes it to the `.__init__()` function for further configuration (that YOU define).  In other words, Python calls `SparkContext.__init__(self, stuff...)`.\n",
    "\n",
    "Finally, `self` is returned and you can assign it to a variable (e.g. `sc` below):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local', 'rdd_tutorial')  # this actually calls the .__init__ method inside the SparkContext class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a little bit of data to create a simple RDD (which is also an object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [542 ,753, 2345 ,7, 3245, 5432, 76, 3]\n",
    "data_rdd = sc.parallelize(data)\n",
    "# data is an object (a list), and so is data_rdd (an RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention in Python, types (aka classes, the blueprints) are usually `CamelCase`.  Variables (aka instances, aka objects) are `snake_case`.\n",
    "\n",
    "The only exception to the `CamelCase` conventions are the built-in types `int`, `float`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD class defines a method (aka member function) `.map()` that we have been using.  Let's use it now to perform a simple transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def increment(x):\n",
    "    return x+1\n",
    "\n",
    "newdata_rdd = data_rdd.map(increment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's call another method, `.collect()`, to bring the transformed data back to the driver as a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[543, 754, 2346, 8, 3246, 5433, 77, 4]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_list = newdata_rdd.collect()\n",
    "result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did this actually work?  The `increment` function was defined HERE (in the driver), but it was somehow \"pushed\" up into Spark and run on the RDD (using `.map`).\n",
    "\n",
    "The answer is called **serialization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serialization in Python\n",
    "\n",
    "*Serialization* is a big topic in data engineering (and programming in general).  The idea is to take something (e.g. data, or a function) that is \"alive\" in memory, snapshot it into raw bytes, and then push it out over the network.\n",
    "\n",
    "The receiving machine can take these bytes and **deserialize** to make it \"alive\" again in its memory.\n",
    "\n",
    "There are a few standard *data* serialization formats in widespread use in data engineering, and they support many languages.  We'll talk about some of these later.\n",
    "\n",
    "For shipping *functions*, however, we need something that is Python-specific.  Python's built-in serialization library is called `pickle` (think of pickling food).  It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "increment_frozen = pickle.dumps(increment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(increment_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(increment_frozen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now ship `increment_frozen` across the network, or write it to disk, or whatever.  It is just bytes.  Now let's pretend we are on the receiving machine.  Let us make it live again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "increment_thawed = pickle.loads(increment_frozen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(increment_thawed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "increment_thawed(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping data in RDDs by key\n",
    "\n",
    "Let's shift back to RDDs and some of their features.\n",
    "\n",
    "One important way to slice and dice data is *grouping*.  For example, we might be interested in gathering statistics by day of week.\n",
    "\n",
    "Let's start with a much simpler problem:  consider the following simple RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_rdd = sc.parallelize([6, 3, 4, 53, 654, 2, 5, 8, 1 , 65, 66, 54])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we wanted to group this data together into two groups:  even and odd numbers.  By definition, an even number has remainder `0` when divided by `2`.  An odd number has remainder `1` when divided by `2`.\n",
    "\n",
    "Python comes with a handy \"remainder\" operator (usually called the **modulo** operator) that will allow us to determine even/oddness.  Here are some examples:\n",
    "```\n",
    "0 % 2 = 0  # The remainder of 0 divided by 2.  Read '0 modulo 2'\n",
    "1 % 2 = 1  # The remainder of 1 divided by 2.  Read '1 modulo 2'\n",
    "2 % 2 = 0  # The remainder of 2 divided by 2.  Read '2 modulo 2'\n",
    "3 % 2 = 1  # and so on\n",
    "4 % 2 = 0\n",
    "5 % 2 = 1\n",
    "...\n",
    "```\n",
    "\n",
    "Consider applying the following function to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6),\n",
       " (1, 3),\n",
       " (0, 4),\n",
       " (1, 53),\n",
       " (0, 654),\n",
       " (0, 2),\n",
       " (1, 5),\n",
       " (0, 8),\n",
       " (1, 1),\n",
       " (1, 65),\n",
       " (0, 66),\n",
       " (0, 54)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd = simple_rdd.map(lambda x: (x % 2, x))\n",
    "key_value_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the new RDD `key_value_rdd` is a collection tuples (each containing 2 elements).  We call these **key-value** pairs in Spark.  The \"key\" is either 0 or 1 (even or odd) in this example, and the \"value\" is just the original piece of data.\n",
    "\n",
    "Another way to create the same thing is to use the `keyBy` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 6),\n",
       " (1, 3),\n",
       " (0, 4),\n",
       " (1, 53),\n",
       " (0, 654),\n",
       " (0, 2),\n",
       " (1, 5),\n",
       " (0, 8),\n",
       " (1, 1),\n",
       " (1, 65),\n",
       " (0, 66),\n",
       " (0, 54)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd = simple_rdd.keyBy(lambda x: x % 2)\n",
    "key_value_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we just wanted to know how many even and odd numbers there are.  We could use `countByKey` to determine this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {0: 7, 1: 5})"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_rdd.countByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even do something more interesting, e.g. sum all of the evens together and sum all of the odds together.  The thing to use here is `reduceByKey`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 794), (1, 127)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_odd_summed_rdd = key_value_rdd.reduceByKey(lambda x,y: x+y)\n",
    "even_odd_summed_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of counting or reducing, we can simply group by key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, <pyspark.resultiterable.ResultIterable at 0x7fc5a082a080>),\n",
       " (1, <pyspark.resultiterable.ResultIterable at 0x7fc5a082a0f0>)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd = key_value_rdd.groupByKey()\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *spirit* your output should look like\n",
    "```\n",
    "[(0, [6, 4, 654, 2, 8]),\n",
    " (1, [3, 53, 5, 1, 65])]\n",
    "```\n",
    "In actuality, the lists are computed lazily, so you should see some nonsense about \"iterable\".  To actually get what we want (lists) we need to apply the `list()` function to each value.\n",
    "\n",
    "Fortunately, Spark provides a `mapValues` function for just such an occasion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [6, 4, 654, 2, 8, 66, 54]), (1, [3, 53, 5, 1, 65])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd = key_value_rdd.groupByKey()\n",
    "grouped_rdd = grouped_rdd.mapValues(list)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get to the same place (skipping the intermediate `key_value_rdd`) is to use `groupBy` instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, [6, 4, 654, 2, 8, 66, 54]), (1, [3, 53, 5, 1, 65])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_rdd = simple_rdd.groupBy(lambda x: x % 2)\n",
    "grouped_rdd = grouped_rdd.mapValues(list)\n",
    "grouped_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many other operations we can perform with keys.  For example, we can `.join()` two RDDs based on matching keys (just like joining tables in SQL).\n",
    "\n",
    "We'll come to that later, though.\n",
    "\n",
    "The documentation listing all the transformations and actions you can apply to an RDD is here:\n",
    "\n",
    "http://spark.apache.org/docs/2.1.0/api/python/pyspark.html#pyspark.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
