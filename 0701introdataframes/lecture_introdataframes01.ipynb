{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Spark DataFrames\n",
    "\n",
    "Recall the progression of \"list-like\" technology in Python:\n",
    "\n",
    "1. `list`\n",
    "2. `numpy` array  (much faster, but limited to single machine)\n",
    "3. Spark RDD  (distributed across many machines)\n",
    "\n",
    "Tabular data has seen a similar progression:\n",
    "\n",
    "1. `dict` (*could* be used to store tabular data)\n",
    "2. `pandas` DataFrame  (more brains, but limited to single machine)\n",
    "3. Spark DataFrame  (distributed across many machines)\n",
    "\n",
    "We want to dive into Spark DataFrames now.\n",
    "\n",
    "\n",
    "## Warmup: `dict` and `pandas`\n",
    "\n",
    "FWIW, here are two ways we can use dicts to store tabular data (but this has no brains):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"columnar\" storage (dict of lists)\n",
    "columnar_table = {\"col1\": [3, 5, 4],\n",
    "              \"col2\": [6.23, 4.2, 6.8]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"row\" storage (list of dicts)\n",
    "row_table = [{\"col1\": 3, \"col2\": 6.23},\n",
    "             {\"col1\": 5, \"col2\": 4.2},\n",
    "             {\"col1\": 4, \"col2\": 6.8}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pandas` can turn either one of these into the a DataFrame, but the result will ALWAYS be columnar (each column = a `numpy` array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     3  6.23\n",
       "1     5  4.20\n",
       "2     4  6.80"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = pd.DataFrame(columnar_table)\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     3  6.23\n",
       "1     5  4.20\n",
       "2     4  6.80"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame(row_table)\n",
    "pandas_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to get the raw `pandas` column (which will be a `numpy` array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1 = pandas_df['col1'].values\n",
    "type(col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrames\n",
    "\n",
    "Let's move onto Spark DataFrames.  These are somewhat similar to `pandas` DataFrames, except they are distributed.\n",
    "\n",
    "Another difference:  in Spark, DataFrames are stored in row-based format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for DataFrame we use a SparkSession instead of a SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession uses the \"builder\" syntax\n",
    "ss = SparkSession.builder.\\\n",
    "     config(\"fs.defaultFS\", \"hdfs://namenode:8020\").\\\n",
    "     config(\"dfs.replication\", \"1\").\\\n",
    "     master('spark://spark-master:7077').\\\n",
    "     appName('test').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These examples are very similar in spirit to `sc.parallelize` for RDDs.  We are pushing small amounts of data up into Spark, usually for demonstration purposes:\n",
    "\n",
    "### from list of dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "## Spark defaults to row-based, so we have to feed it row-by-row\n",
    "df = ss.createDataFrame([Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from `pandas`\n",
    "\n",
    "We can also create from a `pandas` DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.createDataFrame(pandas_df)\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from list of tuples\n",
    "\n",
    "Another easy way to create a DataFrame is from a list of tuples, passing the column names explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.createDataFrame([(3, 6.23), (5, 4.2), (4, 6.8)], [\"col1\", \"col2\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### from RDD\n",
    "\n",
    "RDDs look VERY similar to a list of tuples, so it shouldn't surprise you that we can create a DataFrame from an RDD using almost exactly the same syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# under the hood a SparkSession actually uses a SparkContext\n",
    "# we can get the underlying SparkContext in order to play with RDDs\n",
    "sc = ss.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([(3, 6.23), (5, 4.2), (4, 6.8)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = ss.createDataFrame(rdd, [\"col1\", \"col2\"])\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to RDD\n",
    "\n",
    "We can also convert a DataFrame back to an RDD (but an RDD of `Row`s).\n",
    "\n",
    "`Row` and `Column` are classes out of which the `DataFrame` class is built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = df.rdd\n",
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to `pandas`\n",
    "\n",
    "We can convert back to a `pandas` DataFrame (this brings all the data back to the driver, basically like a `.collect()`, so be careful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>col1</th>\n",
       "      <th>col2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>6.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>4.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>6.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   col1  col2\n",
       "0     3  6.23\n",
       "1     5  4.20\n",
       "2     4  6.80"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df2 = df.toPandas()\n",
    "pandas_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## take vs head vs show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)  # to mimic the `.head()` method in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|col1|col2|\n",
      "+----+----+\n",
      "|   3|6.23|\n",
      "|   5| 4.2|\n",
      "|   4| 6.8|\n",
      "+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schemas\n",
    "\n",
    "In all of the examples above the schema was *inferred*.  Spark just looked at the data and decided what type it should be.  Spark might make a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('col1', 'bigint'), ('col2', 'double')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes  # these are Pythonic numpy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(col1,LongType,true),StructField(col2,DoubleType,true)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema  # these are Java-esque \"equivalent\" types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I intended for `col1` to not get very large?  It might be more space efficient for me to store it as a single `byte`.\n",
    "\n",
    "(aside:  `double` is common typename in other languages for `float`.  Remember that Spark is implemented in Scala under the hood.  There are actually quite a few basic types)\n",
    "\n",
    "We can specify a *schema*.  However, it is slightly verbose.  Remember that we are interfacing into the Java/Scala world, which is notoriously verbose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, ByteType, DoubleType\n",
    "\n",
    "schema = StructType([StructField(\"col1\", ByteType(), True),\n",
    "                     StructField(\"col2\", DoubleType(), True)])\n",
    "\n",
    "# the True arguments above specify whether or not the data is\n",
    "# allowed to be missing (null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(col1=3, col2=6.23), Row(col1=5, col2=4.2), Row(col1=4, col2=6.8)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_schema = ss.createDataFrame([(3, 6.23), (5, 4.2), (4, 6.8)], schema)\n",
    "df_from_schema.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('col1', 'tinyint'), ('col2', 'double')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_schema.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(col1,ByteType,true),StructField(col2,DoubleType,true)))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_from_schema.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing out\n",
    "\n",
    "We can write out data out to HDFS.  For example, here is how to write out to a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vagrant'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# be careful that we are running as a user that has permission to\n",
    "# write to a directory in HDFS\n",
    "sc.sparkUser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['structured-2018-01-14-neworleans',\n",
       " 'structured-2018-03-11-atlanta',\n",
       " 'structured-2018-04-01-birmingham',\n",
       " 'structured-2018-04-08-proleague1',\n",
       " 'structured-2018-04-19-relegation',\n",
       " 'structured-2018-04-22-seattle',\n",
       " 'structured-2018-06-17-anaheim',\n",
       " 'structured-2018-07-29-proleague2',\n",
       " 'structured-2018-08-19-champs']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hdfs import InsecureClient\n",
    "client = InsecureClient(\"http://namenode:50070\", user='vagrant')\n",
    "client.list('/Users/vagrant')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.csv('hdfs://namenode/Users/vagrant/test_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet is very popular, and much more efficient than csv\n",
    "df.write.parquet('hdfs://namenode/Users/vagrant/test_df.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incidentally, I never showed you how to write out an RDD.  It is just as easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.saveAsPickleFile('hdfs://namenode/Users/vagrant/test_rdd.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
